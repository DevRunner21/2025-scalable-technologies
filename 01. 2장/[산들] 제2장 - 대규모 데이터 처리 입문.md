# 제2장 - 대규모 데이터 처리 입문

## 4강 - 하테나 북마크의 데이터 규모

> 데이터가 많을수록 처리에 시간이 걸린다.
> 

### 하테나 북마크를 예로 본 대규모 데이터

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image.png)

하테나 북마크의 어느 엔트리에 어떤 키워드가 포함되어 있는지를 나타내는 DB 테이블 건수를 출력한 것이다. 3.5억 개의 레코드가 있기 때문에 `select * from relword` 쿼리로는 결과가 나오지 않는다.

### 하테나 북마크의 데이터 규모

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%201.png)

Google이나 야후는 테라바이트, 페타바이트가 되므로 이는 초대형규모이며, 이와 비교하면 하테나는 대규모~중규모 정도다. 그렇지만 보통 웹 애플리케이션을 만들면서 좀처럼 기가바이트 단위의 DB는 나오지 않으므로 일반적인 웹 애플리케이션이라는 관점에서 보면 큰 규모다. 하테나 북마크는 일본에서 최대의 소셜 북마크 서비스로, 사용자 수에서도 큰 규모라고 할 수 있다.

### 대규모 데이터로의 쿼리 - 대규모 데이터를 다루는 감각

표 2.1에서 본 정도의 규모인 DB가 되면 쿼리를 던져 결과가 나올 때의 느낌도 달라진다. 

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%202.png)

1건의 레코드를 조회하는 쿼리도 인덱스를 태우지 않으면 200초를 기다려도 결과가 출력되지 않는다. 이것이 이 책에서 대상으로 하는 대규모 데이터에 대한 예다. 

레코드 수가 수천만 건 단위, 데이터 크기는 수 GB부터 수백 GB, 이 정도의 데이터가 되면 아무 생각 없이 던진 쿼리에 대해 응답하지 않는다. 디버그 목적으로 데이터를 출력해보니 엄청난 부하가 걸렸다는 말도 장난이 아닌 상황이다. 

데이터가 많으면 처리하는 데 시간이 걸리게 된다. 직감으로는 알겠으나 왜 그런지를 이해해두는 것이 중요하다. 

## 5강 - 대규모 데이터 처리의 어려운 점

> 메모리와 디스크
> 

### 대규모 데이터는 어떤 점이 어려운가? - 메모리 내에서 계산할 수 없다.

대규모 데이터를 다룰 때 어려운 점이라고 생각되는 포인트가 몇가지 있다.

첫번째로는 ‘**메모리 내에서 계산할 수 없다**’는 점이다. 메모리 내에서 계산할 수 있다면, 아무리 무식한 방법으로 하더라도 그런대로 계산은 이뤄져서 200초나 기다리는 일은 없을 거다. 강의 4에서 본 예 정도의 규모가 되면 데이터가 너무 많아서 메모리 내에서 계산할 수 없으므로 디스크에 두고 특정 데이터를 검색하게 된다. 그리고 **디스크는 메모리에 비해 상당히 느리다**.

### 메모리와 디스크의 속도차 - 메모리는 10^5~10^6배 이상 고속

메모리는 디스크보다 10만~100만 배 정도 빠르다. 이런 수치 감각은 꽤 중요하다.

### 디스크는 왜 늦을까? - 메모리와 디스크

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%203.png)

우선 메모리는 전기적인 부품이므로 물리적 구조는 탐색속도와 그다지 관계없다. 예를 들어 ②(값=0)를 탐색하다가 ①(값=5)부분을 확인하고자 할 때에도 마이크로초 단위로 포인터를 이동시킬 수 있다.

디스크는 동축 상에 ‘원반’이 쌓여있다. 이 원반이 회전하고 있고 여기서 데이터를 읽어낸다. 즉, 메모리와는 달리 회전 등의 물리적인 동작을 수반하고 있다. 이 물리적인 구조가 탐색 속도에 영향을 준다. 예를 들면 ④부근데 데이터가 1바이트 저장되어 있다고 하자. 그 옆에는 자기를 읽어들이는 ‘헤드’가 있다. 이 헤드가 달라붙는지 아닌지에 따라 자기를 읽어내 데이터를 읽어내는 구조로 되어 있다. 먼저 이 헤드를 ④의 원반 상의 바깥쪽 위치에서 원반 안쪽, 즉 읽어야 할 데이터가 놓여 있는 위치인 ③으로 옮기는 작업이 필요하다. 헤드는 마이크로초, 밀리초의 속도로 움직이기 때문에 빠르다고 할 수 없다.

더구나 여기서 데이터를 읽을 때 ⑤와 같이 회전하고 있다면 원반 상의 ③ 위치가 이미 헤드보다 조금 앞으로 가버려서 원하는 위치를 읽기 위해 원반을 한 바퀴 더 돌려야 한다. 

메모리는 1회 탐색 시 마이크로초면 되지만 디스크는 **헤드의 이동**과 **원반의 회전**이라는 두 가지 물리적인 이동이 필요하기 때문에 수 밀리초가 걸리게 된다. 

> 2025년에도 사용되는 보조기억장치는 대부분 HDD?
> 

### OS 레벨에서의 연구

디스크는 느리지만 OS는 이것을 어느정도 커버하는 작용을 한다. ⑦과 같이 OS는 **연속된 데이터를 같은 위치에 쌓는다**. 그리고 나서 데이터를 읽을 때 1바이트씩 읽는 것이 아니라 **4KB정도를 한 번에 읽는다**. 

이렇게 해서 비슷한 데이터를 비슷한 곳에 두어 1번의 디스크 회전으로 읽는 데이터 수를 많게 한다. 그 결과로 디스크의 회전을 최소화할 수 있다.

### 전송속도, 버스의 속도차

지금까지는 탐색속도 측면에서 메모리가 디스크에 비해 10^5~10^6배 이상 빠르다는 얘기였는데 이뿐만이 아니다.

**전송속도**도 차이가 난다. 메모리나 디스크 모두 CPU와 **버스**로 연결되어 있다. (그림⑧,⑨) 이 버스의 속도에서도 상당한 치가 있다. 찾은 데이터를 CPU로 전송하기 위한 속도다. 

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%204.png)

`hdparm`이라는 리눅스 툴을 사용하면 그 속도차를 알 수 있다. ‘**Timing cached reads**’는 메모리 전송속도, ‘**Timing buffered dis reads**’는 디스크의 전송속도이다. 대략 100배 정도 차이가 난다. 

SSD는 물리적인 회전이 아니므로 탐색은 빠르지만 버스 속도가 병목이 되거나 그 밖에 구조에 기인하는 면(?)이 있어서 메모리만큼의 속도는 나오지 않는다.

(현대의 컴퓨터에서는 메모리와 디스크 속도차를 생각하고 애플리케이션을 만들어야 한다. 이는 확장성을 생각할 때 매우 본질적이면서도 어려운 부분이다.)

> **Linux 단일 호스트의 부하** (서버/인프라를 지탱하는 기술) 요약

단일 서버의 성능을 충분히 끌어낼 수 있는 것을 시작으로 복수 서버에서의 부하분산이 의미를 갖는다.

**추측하지 말라, 계측하라 … 단일 호스트의 성능 끌어내기**
단일 호스트의 성능을 끌어내는 데에는 서버 리소스 이용현황을 정확하게 파악할 필요가 있다. 즉, 부하가 어느 정도 걸리고 있는지를 조사할 필요가 있다. 그리고 이런 계측작업이야말로 단일 호스트의 부하를 줄이는 데 가장 중요한 작업이다. 

**병목 규명작업의 기본적인 흐름**
- Load Average 확인
- CPU, I/O 중 병목 원인 조사

**Load Average 확인**
부하 규명의 시작이 되는 지표로 top나 uptime 등의 명령으로 Load Average를 확인한다. Load Average는 시스템 전체의 부하상황을 나타내는 지표다. 다만 Load Average만으로는 병목의 원인이 어딘지를 판단할 수 없다. Load Average 값을 시작으로 병목지점을 조사한다.
Load Average이 낮은데 시스템의 전송량이 오르지 않을 경우도 가끔있다.

**CPU, I/O 중 병목 원인 조사**
Load Average가 높은 경우, CPU와 I/O 중 어디에 원인이 있는지 조사한다. 
`sar`이나 `vmstat`로 시간 경과에 따라 CPU 사용률이나 I/O 대기율의 추이를 확인할 수 있으므로 이를 참고해서 규명한다. 

**CPU 부하가 높은 경우**
> 
> - 사용자 프로그램의 처리가 병복인지, 시스템 프로그램이 원인인지를 확인한다. `top`이나
> `sar`로 확인한다.
> - 또한 `ps`로 볼 수 있는 프로세스의 상태나 `CPU` 사용시간 등을 보면서 원인이 되고 있는
> 프로세스를 찾는다.
> - 프로세스를 찾은 후 보다 상세하게 조사할 경우는 `strace`로 추적하거나 `oprofile`로 프
> 로파일링을 해서 병복지점을 좁혀간다.
> 
> 일반적으로 CPU에 부하가 걸리고 있는 것은 다음 상황 중 하나다.
> 
> - 디스크나 메모리 용량 등 그 밖의 부분에서는 병목이 되지 않는, 말하자면 이상적인 상태
> - 프로그램이 폭추해서 CPU에 필요이상의 부하가 걸리는 경우
> 
> 전자의 상태에다 시스템의 전송량에 문제가 있다면 서버 증설이나 프로그램의 로직이나
> 알고리즘을 개선해서 대응한다. 
> 
> 후자의 경우는 오류를 제거해서 프로그램이 폭주하지 않도록 대처한다.
> 
> **I/O 부하가 높은 경우**
> 
> 프로그램으로부터 입출력이 많아서 부하가 높거나 스왑이 발생해서 디스크 액세스가 발생하고 있는 상황 중 하나일 경우가 대부분이다. `sar`나 `vmstat`로 스왑의 발생상황을 확인해서 문제를 가려낸다.
> 
> 확인한 결과 스왑이 발생하고 있을 경우는 다음과 같은 점을 실마리로 조사한다.
> 
> - 특정 프로세스가 극단적으로 메모리를 소비하고 있지 않은지를 ps로 확인할 수 있다.
> - 프로그램의 오류로 메모리를 지나치게 사용하고 있는 경우에는 프로그램을 개선한다.
> - 탑재된 메모리가 부족한 경우에는 메모리를 증설한다. 메모리를 증설할 수 없을 경우는
> 분산을 검토한다.
> 
> 스왑이 발생하지 않고 디스크로의 입출력이 빈번하게 발생하고 있는 상황은 캐시에 필요
> 한 메모리가 부족한 경우를 생각해볼 수 있다. 해당 서버가 저장하고 있는 데이터 용량과 증
> 설 가능한 메모리 양을 비교해서 다음과 같이 나눠서 검토한다.
> 
> - 메모리 증설로 캐시영역을 확대시킬 수 있는 경우는 메모리를 증설한다.
> - 메모리 증설로 대응할 수 없는 경우는 데이터 분산이나 캐시서버 도입 등을 검토한다. 물론, 프로그램을 개선해서 V/O 빈도를 줄이는 것도 검토한다.
> 
> **OS 튜닝이란 부하의 원인을 알고 이것을 제거하는 것**
> 
> 튜닝의 본래 의미는 '병목이 발견되면 이를 제거하는' 작업이다. 애초에 본래 하드웨어나 소프트웨어가 지니고 있는 성능 이상의 성능을 내는 것은 아무리 노력해도 불가능하다. 할 수 있는 것은 '하드웨어/소프트웨어가 본래 지닌 성능을 충분히 발휘할 수 있도록 문제가 될 만한 부분이 있다면 제거하는' 것이다.
> 
> 최근의 OS나 미들웨어는 디폴트 상태에서도 충분한 성능을 발휘할 수 있도록 설정되어있다. 정체되지 않은 고속도로의 차도를 넓혀도 1대의 자동차가 목적지에 도달하기까지 걸리는 시간은 달라지지 않는 것과 마찬가지로, 디폴트 설정이 최적화되어 있다면 아무리 설정을 변경하더라도 대부분의 경우에는 효과가 없다. 
> 
> 예를 들면, CPU의 계산시간을 최대한 이용해서 10초 걸리는 처리는 아무리 OS 설정을 만진다고 해도 10초 이하로 줄어들 수는 없다. 이것이 정체되지 않은 고속도로의 예다. 한 편, 예를 들어 다른 프로그램의 I/O 성능이 영향을 끼치고 있어서 해당 프로그램이 본래 10초에 끝낼 일을 100초 걸려 마친다고 할 경우에는 I/O 성능을 개선할 수 있다. 이는 정체하고 있는 고속도로의 예다. /O 성능을 개선하기 위해서는 다음과 같은 것을 규명할 필요가 있다.
> 
> - 메모리를 증설해서 캐시 영역을 확보함으로써 대응할 수 있는가
> - 원래 데이터량이 너무 많지는 않은가
> - 애플리케이션 측의 I/O 알고리즘을 변경할 필요가 있는가
> 
> 결국 원인을 알면 해당 원인에 대한 대응방법은 자명하다. 이렇게 자명해진 대응방법을 실천하는 것이 튜닝이다.
> 

## 6강 - 규모조정의 요소

### 규모조정, 확장성

데이터가 커지면 메모리와 디스크의 속도차에 기인하는 문제가 복잡해지기 쉽다. 강의 6에서는 이런 사항들이 시스템 전체의 확장성 전략에 어떤 영향을 줄지에 대해 살펴본다.

대규모 환경이라고 하면 서버를 여러 대 놓고 그 서버로 부하를 분산한다. 웹 서비스에서 자주 거론되는 **규모조정(scaling)**, **확장성(scalability)**은 그런 종유의 얘기다. 

웹 서비스에서는 고가의 빠른 하드웨어를 사서 성능을 높이는 ‘**스케일업**’ 전략보다도 저가이면서 일반적인 성능의 하드웨어를 많이 나열해서 시스템 전체 성능을 올리는 ‘**스케일아웃**’ 전략이 주류다. 스케일아웃 전략이 더 나은 이유는 웹 서비스에 적합한 형태이고 비용이 저렴하다는 점과 시스템 구성에 유연성이 있다는 점이 포인트다. 

### 규모조정의 요소 - CPU 부하와 I/O부하

스케일아웃은 하드웨어를 나열해서 성능을 높이는데 이때 CPU 부하의 확장성을 확보하기는 쉽다. 예를 들어 웹 앱에서 HTTP 요청을 받아 DB에 질의하고 응답을 반환할 때는 기본적으로 CPU 부하만 소요된다. 한 편 DB 서버 측면에서는 I/O 부하가 걸린다. 이는 대규모 데이터 및 DB라는 두 가지 관점에서 자세히 살펴보자.

### 웹 애플리케이션과 부하의 관계

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%205.png)

웹 앱과 부하의 관계를 살펴보자. 웹 앱의 3단 구조에는 **프록시**, **AP서버**, **DB**가 있다. 요청이 오면 DB를 조회해서 응답을 반환하는 간단한 흐름이다. AP서버에는 I/O 부하가 걸리지 않고 DB 측에 I/O 부하가 걸린다.

AP 서버는 CPU 부하만 걸리므로 분산이 간단하다. 데이터를 분산해서 가지고 있지 않기 때문에 여러 서버가 동일하게 작업을 처리하기만 하면 분산할 수 있다. 따라서 대수를 늘리기만 하면 간단히 확장할 수 있다. 요청을 균등하게 분산하는 것은 **로드밸런서**가 한다.

한편 I/O 부하는 문제가 있다. DB를 추가하게 되면 **데이터의 동기화** 문제가 생긴다. 

### DB 확장성 확보의 어려움

7강에서 자세히 알아보자.

> **두 종류의 부하와 웹 애플리케이션** (서버/인프라를 지탱하는 기술) 요약
> 
> 
> 
> 앞서 말했듯이 일반적으로 부하는 크게 두 가지로 분류된다.
> 
> - CPU 부하
> - I/O 부하
> 
> 예를 들어 대규모의 과학계산을 수행하는 프로그램이 있는데, 이 프로그램은 I/O는 하지 않지만 처리가 완료될 때까지 상당한 시간을 요한다고 하자. '계산을 한다' 라는 것으로부터 생각할 수 있듯이, 이 프로그램의 처리속도는 CPU의 계산속도에 의존하고 있다. 이것이 바로 CPU에 부하를 주는 프로그램이다. '**CPU 바운드한 프로그램**' 이라고도 한다.
> 
> 한편, 디스크에 저장된 대량의 데이터에서 임의의 문서를 찾아내는 검색 프로그램이 있다고하자. 이 검색 프로그램의 처리속도는 CPU가 아닌, 디스크의 읽기속도, 즉 입출력에 의존할 것이다. 디스크가 빠르면 빠를수록 검색에 걸리는 시간은 짧아진다. I/O에 부하를 주는 종류의 프로그램이라고 해서 ‘**I/O 바운드한 프로그램**' 이라고 한다.
> 
> 일반적으로 AP 서버는 DB로부터 얻은 데이터를 가공해서 클라이언트로 전달하는 처리를 수행한다. 그 과정에서 대규모 I/O를 발생시키는 일은 드물다. 따라서 많은 경우에 AP 서버는 CPU 바운드한 서버라고 할 수 있다. 
> 
> 반면, 웹 애플리케이션을 구성하는 또 하나의 요소 시스템인 DB 서버는 데이터를 디스크로 부터 검색하는 것이 주된 일로, 특히 데이터가 대규모가 되면 될수록 CPU에서의 계산시간보다도 I/O에 대한 영향이 커지는 I/O 바운드한 서버다. 같은 서버라도 부하의 종류가 다르면 그 특성은 크게 달라진다.
> 
> **멀티태스킹 OS와 부하**
> 
> ![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%206.png)
> 
> Windows나 Linux 등 최근의 멀티태스킹 OS는 그 이름처럼 동시에 서로 다른 여러 태스크=처리를 실행할 수 있다. 그러나 여러 태스크를 실행한다고 해도 실제로는 CPU나 디스크 등 유한한 하드웨어를 그 이상의 태스크에서 공유할 필요가 있다. 그래서 매우 짧은 시간 간격으로 **여러 태스크를 전환**(컨텍스트 스위칭)해가면서 처리함으로써 멀티태스킹을 실현하고 있다.
> 
> 실행할 태스크가 적은 상황에서 OS는 태스크에 대기를 발생하지 않고 전환을 할 수 있다. 그러나 실행할 태스크가 늘어나면 특정 태스크 A가 CPU에서 계산을 수행하고 있는 동안 다음에 계산을 수행하고자 하는 다른 태스크 B나 C는 CPU에 시간이 날 때까지 대기하게 된다. 이렇듯 '처리를 실행하려고 해도 대기한다'라는 대기상태는 프로그램의 실행지연으로 나타난다.
> 
> `top` 의 출력내용에는 'LoadAverage(평균부하)' 라는 수치가 포함되어 있다.
> load average: 0.70, 0.66, 0.59
> 
> Load Average는 왼쪽부터 차례로 1분, 5분, 15분 동안에 단위시간당 대기된 태스크의 수, 즉 평균적으로 어느 정도의 태스크가 대기상태로 있었는지를 보고하는 수치다. LoadAverage가 높은 상황은 그만큼 태스크 실행에 대기가 발생하고 있다는 표시이므로, 지연이되는=부하가 높은 상황이라고 할 수 있다.
> 
> **Average가 보고하는 부하의 정체**
> 
> 하드웨어는 일정 주기로 CPU로 **인터럽트**라고 하는 신호를 보낸다. 주기적으로 보내지는 신호라는 점에서 ' **타이머 인터럽트**' 라고 한다. 예를 들면, CentOS5에서의 인터럽트 간격은 4ms(밀리초)가 되도록 설정되어 있다. 이 인터럽트마다 CPU는 시간을 진행시키거나 실행 중인 프로세스가 CPU를 얼마나 사용했는지를 계산하는 등 시간에 관련된 처리를 수행한다. 이때 타이머 인터럽트마다 Load Average 값이 계산된다.
> 
> 커널은 타이머 인터럽트가 발생했을 때 실행가능 상태인 태스크와 I/O 대기인 태스크의 개수를 세어둔다. 그 값을 단위시간으로 나눈 것이 Load Average 값으로 보고된다. 실행가능 상태인 태스크란 다른 태스크가 CPU를 점유하고 있어 계산을 시작할 수 없는 태스크다. 
> 
> 즉, Load Average가 의미하는 부하는 **처리를 실행하려고 해도 실행할 수 없어서 대기하고 있는 프로세스의 수**를 말하며, 보다 구체적으로는 다음과 같음을 알 수 있다.
> 
> - CPU의 실행권한이 부여되기를 기다리고 있는 프로세스
> - 디스크 I/O가 완료하기를 기다리고 있는 프로세스
> 
> 이것은 분명히 직감과 일치한다. CPU에 부하가 걸릴 것 같은 처리, 예를 들면 동영상 인코딩 등을 수행하고 있는 도중에 다른 동종의 처리를 수행하고자 해도 결과가 늦어지거나 디스크에서 데이터를 대량으로 읽는 동안은 시스템의 반응이 둔해진다. 한편, 키보드 대기 중인 프로세스가 아무리 많더라도 그것을 원인으로 해서 시스템의 응답이 늦는 일은 없다.
> 
> Load Average 자체는 두 가지의 부하를 합쳐서 어디까지나 대기 태스크 수만을 나타내는 수치이므로 이를 보는 것만으로는 CPU 부하가 높은지, I/O 부하가 높은지는 판단할 수 없다. 최종적으로 서버 리소스 중 어디가 병목이 되고 있는지를 판단하려면 좀 더 자세하게 조사할 필요가 있다.
> 

## 7강 - 대규모 데이터를 다루기 위한 기초지식

### 프로그래머를 위한 대규모 데이터 기초

대규모 데이터는 메모리에서 처리하기 어렵고 디스크는 느리다. 또한 분산하기도 곤란하다. 

대규모 데이터를 다루는 방법을 두 가지 관점에서 정리했다.

1. **프로그램을 작성할 때의 요령**
2. **프로그램 개발의 근간이 되는 기초라는 점에서 전제로서 알아두었으면 하는 것**

### 대규모 데이터를 다루는 세 가지 급소 - 프로그램을 작성할 때의 요령

**요령 1. 어떻게 하면 메모리에서 처리를 마칠 수 있을까?**

메모리에서 처리를 마쳐야 하는 이유는 앞서 설명한 대로 디스크 seek 횟수가 확장성, 성능에 크게 영향을 주기 때문이다. 디스크 seek 횟수를 최소화한다는 의미로 메모리를 활용하고자 한다. 국소성을 활용한 분산도 있지만 나중에 살펴본다.

**요령 2. 데이터량 증가에 강한 알고리즘을 사용하자.**

1천만 건의 레코드가 있을 때 선형탐색을 하면 1천만 번의 탐색이 필요하다. Log Order인 알고리즘을 사용하면 수십 번만에 탐색을 마칠 수 있다. 이렇게 알고리즘의 기초적인 부분을 제대로 활용하자.

**요령 3. 데이터 압축이나 검색기술과 같은 테크닉**

단적으로 말하면 압축해서 데이터량을 줄일 수 있다면 읽어내는 seek 횟수도 적어지게 되므로 디스크 읽는 횟수를 최소화할 수 있다. 또한 메모리에 캐싱하기도 쉬워진다. 

검색이 중요한 이유는 확장성 면에서 DB에만 맡겨서 해결할 수 없을 때, 특정 용도에 특화된 검색엔진 등을 만들어서 해당 검색 시스템을 웹 앱에서 사용하는 형태로 전환한다면 속도를 제대로 확보할 수 있다. 

### 대규모 데이터를 다루기 전 3대 전제지식 - 프로그램 개발의 한 층 아래 기초

1. **OS 캐시**
2. **분산을 고려해서 RDBMS를 운용할 때는 어떻게 해야 하는가**
3. **대규모 환경에서 알고리즘과 데이터 구조를 사용한다는 것**

---

**Load Average 다음은 CPU 사용률과 I/O 대기율** (서버/인프라를 지탱하는 기술) 요약

이전 컬럼(45쪽)에서 Load Average의 구체적인 산출방법을 살펴보면 그 값이 CPU 부하
와 I/O 부하를 나타내고 있음을 알 수 있었다. 이는 반대로 말하면, 과부하로 시스템의 성능이
떨어지는 원인은 대부분의 경우에 CPU나 I/O에 있음을 나타내고 있다. 따라서 Load
Average를 보고 대응할 필요가 있는 경우, 다음 단계로 CPU와 I/O 중 어느 쪽에 원인이 있는지 조사해야 한다.

**sar로 CPU 사용률, I/O 대기율 확인**

 여기서 CPU 사용률과 I/O 대기 비율이라는 지표를 활용한다. `sar` 명령어로 확인할 수 있다. `sar`(System Activity Repoter)은 이름 그대로 시스템 상황 리포트를 열람하는 명령이다.

**CPU 바운드인 경우의 sar**

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%207.png)

그림 C.1은 CPU 바운드한 시스템의 sar 결과다. sar의 장점은 부하의 지표를 시간경과에 따라 비교해서 열람할 수 있다. 00:00~00:40 동안 CPU 사용률 추이다. ‘`%user`’는 사용자 모드에서의 CPU 사용률, ‘`%system`’은 시스템 모드에서의 CPU 사용률이다. Load Average가 높고 CPU 사용률이 높다면 대기하고 있는 프로세스의 부하 원인은 CPU 리소스 부족이라고 판단할 수 있다.

**I/O 바운드인 경우의 sar**

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%208.png)

‘`%iowait`’은 I/O 대기율이다. Load Average가 높고 이 값이 높은 경우 부하의 원인이 I/O에 있다고 볼 수 있다. 

CPU, I/O 중 어느 쪽에 원인이 있는지를 확인했으면 거기서부터 더욱 자세하게 조사해가기 위해서 다른 지표, 예를 들어 메모리 사용률이나 스왑 발생상황 등을 참조한다.

이와 같은 병목을 규명할 때는 Load Average 등의 종합적인 수치에서부터 CPU 사용률이나 I/O 대기율 등 보다 구체적인 수치, 나아가서는 각 프로세스의 상태까지 top-down 방식으로 살펴보는 전략이 유효하다.

**멀티 CPU와 CPU 사용률**

근래의 x86 CPU 아키텍처는 멀티코어화가 진행되고 있다. 멀티코어가 되면, CPU가 물리적으로 하나여도 OS에서는 여러 CPU가 탑재되어 있는 것처럼 보인다. Linux 커널은 CPU 사용률 통계를 각각의 CPU 별로 유지하도록 되어 있다.

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%209.png)

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%2010.png)

sar의 -P 옵션을 이용한다. 코어가 4개인 쿼드코어 CPU가 탑재된 서버의 sar 결과다. 각 CPU(코어) 마다 사용률 통계가 얻어지고 있다.

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%2011.png)

I/O 바운드인 서버의 결과를 보자. I/O 대기가 평균적으로 20% 전후다. 이 서버는 코어가 2개인 듀얼코어 CPU를 사용한다. 

![image.png](%E1%84%8C%E1%85%A62%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%83%E1%85%A2%E1%84%80%E1%85%B2%E1%84%86%E1%85%A9%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20%E1%84%8B%E1%85%B5%E1%86%B8%E1%84%86%E1%85%AE%E1%86%AB%201b743171511780899ffae213b9b82c0e/image%2012.png)

sar -P로 코어별 현황을 보면 I/O 대기는 거의 CPU 0번에서만 발생하고 CPU 1번은 거의 작업을 하지 않는다. (’%idle’이 거의 100)

멀티 CPU가 탑재되어 있더라도 디스크는 하나밖에 없는 경우, **CPU 부하는 다른 CPU로 분산돼도 I/O 부하는 분산되지 않는다**. 평균하면 I/O 대기는 20% 정도로 그다지 많지 않은 듯이 보이지만, CPU 별로 보면 그 값의 편중이 현저하게 나타난다. 그렇기 때문에 **멀티코어 환경에서는 경우에 따라서 CPU 사용률울 개별적으로 확인할 필요**가 있다.